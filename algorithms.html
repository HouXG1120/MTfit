

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="English" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="English" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>8. MTfit.algorithms: Search Algorithms &mdash; MTfit documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/style.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="9. convert" href="mtconvert.html" />
    <link rel="prev" title="7. Probability" href="probability.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> MTfit
          

          
            
            <img src="_static/MTfitsphinx.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                1.0.5
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="setup.html">2. Installing</a></li>
<li class="toctree-l1"><a class="reference internal" href="run.html">3. Running MTfit</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial.html">4. Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="real-tutorial.html">5. Tutorial: Real Data Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes.html">6. Bayesian Approach</a></li>
<li class="toctree-l1"><a class="reference internal" href="probability.html">7. Probability Density Functions</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">8. Search Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#table-of-contents">8.1. Table Of Contents:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#algorithms">8.2. Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="#random-monte-carlo-sampling">8.3. Random Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#markov-chain-monte-carlo-sampling">8.4. Markov chain Monte Carlo sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#reversible-jump-markov-chain-monte-carlo-sampling">8.4.1. Reversible Jump Markov chain Monte Carlo Sampling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#reversible-jump-mcmc-in-source-inversion">8.4.1.1. Reversible Jump McMC in Source Inversion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#relative-amplitude">8.5. Relative Amplitude</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-time">8.6. Running Time</a></li>
<li class="toctree-l2"><a class="reference internal" href="#summary">8.7. Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="mtconvert.html">9. Moment Tensor Conversion</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">10. Command Line Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtplot.html">11. Plotting</a></li>
<li class="toctree-l1"><a class="reference internal" href="plot_classes.html">1. Plot Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtplotcli.html">12. MTplot Command Line Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="inversion.html">13. Inversion Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="extensions.html">14. Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">15. References</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">2. Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="source.html">3. Source Code</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/djpugh/MTfit">GitHub Repository</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MTfit</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>8. MTfit.algorithms: Search Algorithms</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/algorithms.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="mtfit-algorithms-search-algorithms">
<h1>8. MTfit.algorithms: Search Algorithms<a class="headerlink" href="#mtfit-algorithms-search-algorithms" title="Permalink to this headline">¶</a></h1>
<p>These algorithms and their effects are explored in <a class="reference internal" href="references.html#pugh-2015t"><span class="std std-ref">Pugh (2015)</span></a>, which expands in further detail on the topics covered here.</p>
<div class="section" id="table-of-contents">
<h2>8.1. Table Of Contents:<a class="headerlink" href="#table-of-contents" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#mcsampling"><span class="std std-ref">Random Monte Carlo sampling</span></a></li>
<li><a class="reference internal" href="#mcmcsampling"><span class="std std-ref">Markov chain Monte Carlo sampling</span></a></li>
</ul>
</div></blockquote>
</div>
<span class="target" id="module-MTfit.algorithms"></span><div class="section" id="algorithms">
<h2>8.2. Algorithms<a class="headerlink" href="#algorithms" title="Permalink to this headline">¶</a></h2>
<p>This module contains the sampling approaches used. Two main approaches are used:</p>
<blockquote>
<div><ul class="simple">
<li>Random Monte Carlo sampling</li>
<li>Markov chain Monte Carlo sampling</li>
</ul>
</div></blockquote>
<p>However, there are also two variants of the Markov chain Monte Carlo (McMC) method:</p>
<blockquote>
<div><ul class="simple">
<li>Metropolis-Hastings</li>
<li>Trans-Dimensional Metropolis-Hastings (Reversible Jump)</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="random-monte-carlo-sampling">
<span id="mcsampling"></span><h2>8.3. Random Monte Carlo sampling<a class="headerlink" href="#random-monte-carlo-sampling" title="Permalink to this headline">¶</a></h2>
<p>The simplest approach is that of random sampling over the moment tensor or double-couple space. Stochastic Monte Carlo sampling introduces no biases and provides an estimate for the true PDF, but requires a sufficient density of sampling to reduce the uncertainties in the estimate. The sampled PDF approaches the true distribution in the limit of infnite samples. However, this approach is limited both by time and memory. Some benefits can be gained by only keeping the samples with non-zero probability.</p>
<p>The assumed prior distribution is a uniform distribution on the unit 6-sphere in moment tensor space. This is equivalent to unit normalisation of the moment tensor six vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{\tilde{M}}  =   \left(\begin{array}{c}M_{11}\\M_{22}\\M_{33}\\\sqrt{2}M_{12}\\\sqrt{2}M_{13}\\\sqrt{2}M_{23}\end{array}\right).\end{split}\]</div>
<p>This sampling is explored further in <a class="reference internal" href="references.html#pugh-2015t"><span class="std std-ref">Pugh et al, 2015t</span></a>.</p>
</div>
<div class="section" id="markov-chain-monte-carlo-sampling">
<span id="mcmcsampling"></span><h2>8.4. Markov chain Monte Carlo sampling<a class="headerlink" href="#markov-chain-monte-carlo-sampling" title="Permalink to this headline">¶</a></h2>
<p>An alternative approach is to use Markov chain Monte Carlo (McMC) sampling. This constructs a Markov chain (<a class="reference internal" href="references.html#norris-1998"><span class="std std-ref">Norris, 1998</span></a>) of which the equilibrium distribution is a good sample of the target probability distribution.</p>
<p>A Markov chain is a memoryless stochastic process of transitioning between states. The probability of the next value depends only on the current value, rather than all the previous values, which is known as the Markov property (<a class="reference internal" href="references.html#markov-1954"><span class="std std-ref">Markov, 1954</span></a>):</p>
<div class="math notranslate nohighlight">
\[\mathrm{p}\left({d_{n}|d_{n-1},d_{n-2},d_{n-3},\ldots d_{0}}\right)=\mathrm{p}\left({d_{n}|d_{n-1}}\right).\]</div>
<p>A suitable McMC method should converge on the target distribution rapidly. As an approach it is more complex than the Monte Carlo random sampling approach described above, and by taking samples close to other non-zero samples, there is moreintelligence to the sampling than in the random Monte Carlo sampling approach.</p>
<p>A Metropolis-Hastings approach is used here (<a class="reference internal" href="references.html#metropolis-1953"><span class="std std-ref">Metropolis, 1953</span></a> and <a class="reference internal" href="references.html#hastings-1970"><span class="std std-ref">Hastings, 1970</span></a>). The Metropolis-Hastings approach is a common method for McMC sampling and satisfies the detailed balance condition (Robert and Casella, 2004, eq. 6.22), which means that the probability density of the chain is stationary. New samples are drawn from a probability density <span class="math notranslate nohighlight">\(\mathrm{q}\left(\mathbf{x'}|\mathbf{x}_\mathrm{t}\right)\)</span> to evaluate the target probability density <span class="math notranslate nohighlight">\(\mathrm{p}\left(\mathbf x|\mathbf d\right)\)</span>.</p>
<p>The Metropolis-Hastings algorithm begins with a random starting point and then iterates until this initial state is forgotten (<span class="xref std std-ref">Algorithm</span>). Each iteration evaluates whether a new proposed state is accepted or not. If <span class="math notranslate nohighlight">\(\mathrm{q}\left(\mathbf{x'}|\mathbf{x}_\mathrm{t}\right)\)</span> is symmetric, then the ratio <span class="math notranslate nohighlight">\(\frac{\mathrm{q}\left(\mathbf{x}_\mathrm{t}|\mathbf{x'}\right)}{\mathrm{q}\left(\mathbf{x'}|\mathbf{x}_\mathrm{t}\right)}=1\)</span>. The acceptance, alpha, is given by</p>
<div class="math notranslate nohighlight">
\[\alpha=\mathrm{min}\left(1,\frac{\mathrm{p}\left(\mathbf{x'}|\mathbf d\right)}{\mathrm{p}\left(\mathbf{x}_{\mathrm{t}}|\mathbf d\right)}.\frac{\mathrm{q}\left(\mathbf{x}_\mathrm{t}|\mathbf{x'}\right)}{\mathrm{q}\left(\mathbf{x'}|\mathbf{x}_{\mathrm{t}}\right)}\right),\]</div>
<p>which can be expanded using <a class="reference internal" href="bayes.html"><span class="doc">Bayes' Theorem</span></a> to give the acceptance in terms of the likelihood <span class="math notranslate nohighlight">\(\mathrm{p}\left(\mathbf d|\mathbf x\right)\)</span> and prior <span class="math notranslate nohighlight">\(\mathrm{p}\left(\mathbf x\right)\)</span>,</p>
<div class="math notranslate nohighlight">
\[\alpha    =   \mathrm{min}\left( 1,\frac{\mathrm{p}\left(\mathbf d|\mathbf{x'}\right)\mathrm{p}\left(\mathbf{x'}\right)}{\mathrm{p}\left(\mathbf d|\mathbf{x}_\mathrm{t}\right)\mathrm{p}\left(\mathbf{x}_\mathrm{t}\right)}.\frac{\mathrm{q}\left(\mathbf{x}_t|\mathbf{x'}\right)}{\mathrm{q}\left(\mathbf{x'}|\mathbf{x}_\mathrm{t}\right)}\right).\]</div>
<p>The acceptance is the probability that the new sample in the chain, <span class="math notranslate nohighlight">\(\mathbf{x}_\mathrm{t+1}\)</span>  is the new sample, <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span>, otherwise the original value, <span class="math notranslate nohighlight">\(\mathbf{x}_\mathrm{t}\)</span> , is added to the chain again,</p>
<div class="math notranslate nohighlight">
\[\begin{split} \mathbf{x_{\mathrm{t}+1}}=\begin{cases}
\mathbf{x'} &amp; probability=\alpha\\
\mathbf{x}_t &amp; probability=1-\alpha
\end{cases}.\end{split}\]</div>
<p>The algorithm used in <code class="xref py py-mod docutils literal notranslate"><span class="pre">MTfit</span></code> is:</p>
<table border="1" class="docutils">
<colgroup>
<col width="3%" />
<col width="2%" />
<col width="94%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head" colspan="3"><strong>Metropolis-Hastings Markov chain Monte Carlo Sampling Algorithm</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>1</td>
<td colspan="2">Determine initial value for <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> with non-zero likelihood</td>
</tr>
<tr class="row-odd"><td>2</td>
<td colspan="2">Draw new sample <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span> from transition PDF <span class="math notranslate nohighlight">\(\mathrm{q}\left(\mathbf{x'}|\mathbf{x}_\mathrm{t}\right)\)</span></td>
</tr>
<tr class="row-even"><td>3</td>
<td colspan="2">Evaluate likelihood for sample <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span></td>
</tr>
<tr class="row-odd"><td>4</td>
<td colspan="2">Calculate acceptance, <span class="math notranslate nohighlight">\(\alpha\)</span>, for <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span>.</td>
</tr>
<tr class="row-even"><td>5</td>
<td colspan="2">Determine sample <span class="math notranslate nohighlight">\(\mathbf{x}_\mathrm{t+1}\)</span>.</td>
</tr>
<tr class="row-odd"><td rowspan="4">6</td>
<td colspan="2">If in learning period:</td>
</tr>
<tr class="row-even"><td><ol class="first last loweralpha simple">
<li></li>
</ol>
</td>
<td>If sufficient samples (&gt; 100) have been obtained, update transition PDF parameters to target ideal acceptance rate.</td>
</tr>
<tr class="row-odd"><td><ol class="first last loweralpha simple" start="2">
<li></li>
</ol>
</td>
<td>Return to 2 until end of learning period and discard learning samples.</td>
</tr>
<tr class="row-even"><td colspan="2">Otherwise return to 2 until sufficient samples are drawn.</td>
</tr>
</tbody>
</table>
<p>The source parameterisation is from <a class="reference internal" href="references.html#tape-2012"><span class="std std-ref">Tape and Tape (2012)</span></a>, and the algorithm uses an iterative parameterisation for the learning parameters during a learning period, then generates a Markov chain from the pdf.</p>
<div class="section" id="reversible-jump-markov-chain-monte-carlo-sampling">
<h3>8.4.1. Reversible Jump Markov chain Monte Carlo Sampling<a class="headerlink" href="#reversible-jump-markov-chain-monte-carlo-sampling" title="Permalink to this headline">¶</a></h3>
<p>The Metropolis-Hastings approach does not account for variable dimension models. <a class="reference internal" href="references.html#green-1995"><span class="std std-ref">Green (1995)</span></a> introduced a new type of move, a <em>jump</em>, extending the approach to variable dimension problems. The jump introduces a dimension-balancing vector, so it can be evaluated like the normal Metropolis-Hastings shift.</p>
<p><a class="reference internal" href="references.html#green-1995"><span class="std std-ref">Green (1995)</span></a> showed that the acceptance for a pair of models <span class="math notranslate nohighlight">\(M_\mathrm{t}\)</span> and <span class="math notranslate nohighlight">\(M'\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\alpha=\min\left(1,\frac{\mathrm{p}\left(\mathbf d|\mathbf{x'},M'\right)\mathrm{p}\left(\mathbf{x'}|M'\right)\mathrm{p}\left(M'\right)}{\mathrm{p}\left(\mathbf d|\mathbf{x}_\mathrm{t},M_\mathrm{t}\right)\mathrm{p}\left(\mathbf{x}_\mathrm{t}|M_\mathrm{t}\right)\mathrm{p}\left(M_\mathrm{t}\right)}.\frac{\mathrm{q}\left(\mathbf{x}_\mathrm{t}|\mathbf{x'}\right)}{\mathrm{q}\left(\mathbf{x'}|\mathbf{x}_\mathrm{t}\right)}\right),\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{q}\left(\mathbf{x'}|\mathbf{x}_\mathrm{t}\right)\)</span> is the probability of making the transition from parameters <span class="math notranslate nohighlight">\(\mathbf{x}_\mathrm{t}\)</span> from model <span class="math notranslate nohighlight">\(M_\mathrm{t}\)</span> to parameters <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span> from model <span class="math notranslate nohighlight">\(M'\)</span>, and <span class="math notranslate nohighlight">\(\mathrm{p}\left(M_\mathrm{t}\right)\)</span> is the prior for the model <span class="math notranslate nohighlight">\(M_\mathrm{t}\)</span>.</p>
<p>If the models <span class="math notranslate nohighlight">\(M_\mathrm{t}\)</span> and <span class="math notranslate nohighlight">\(M'\)</span> are the same, the reversible jump acceptance is the same as the Metropolis-Hastings acceptance, because the model priors are the same. The importance of the reversible jump approach is that it allows a transformation between different models, and even different dimensions.</p>
<p>The dimension balancing vector requires a bijection between the parameters of the two models, so that the transformation is not singular and a reverse jump can occur. In the case where <span class="math notranslate nohighlight">\(\dim\left(M'\right)&gt;\dim\left(M_\mathrm{t}\right)\)</span>, a vector <span class="math notranslate nohighlight">\(\mathbf u\)</span> of length <span class="math notranslate nohighlight">\(\dim\left(M'\right)-\dim\left(M_\mathrm{t}\right)\)</span> needs to be introduced to balance the number of parameters in <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span>. The values of <span class="math notranslate nohighlight">\(\mathbf u\)</span> have probability density <span class="math notranslate nohighlight">\(\mathrm{q}\left(\mathbf u\right)\)</span> and some bijection that maps <span class="math notranslate nohighlight">\(\mathbf{x}_\mathrm{t},\mathbf u\rightarrow\mathbf{x'}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x'}=\mathrm{h}\left(\mathbf{x}_\mathrm{t},\mathbf u\right)\)</span>.</p>
<p>If the jump has a probability <span class="math notranslate nohighlight">\(\mathrm{j}\left(\mathbf x\right)\)</span> of occurring for a sample <span class="math notranslate nohighlight">\(\mathbf x\)</span>, the transition probability depends on the jump parameters, <span class="math notranslate nohighlight">\(\mathbf u\)</span> and the transition ratio is given by:</p>
<div class="math notranslate nohighlight">
\[\frac{\mathrm{q}\left(\mathbf{x}_\mathrm{t},\mathbf u|\mathbf{x'}\right)}{\mathrm{q}\left(\mathbf{x'}|\mathbf{x}_\mathrm{t},\mathbf u\right)} =   \frac{\mathrm{j}\left(\mathbf{x'}\right)}{\mathrm{j}\left(\mathbf{x}_\mathrm{t}\right)\mathrm{q}\left(\mathbf u\right)}\left|\mathbf J\right|,\]</div>
<p>with the Jacobian matrix, <span class="math notranslate nohighlight">\(\mathbf J=\frac{\partial\mathrm{h}\left(\mathbf{x}_\mathrm{t},\mathbf u\right)}{\partial\left(\mathbf{x}_\mathrm{t},\mathbf u\right)}\)</span>.</p>
<p>The general form of the jump acceptance involves a prior on the models, along with a prior on the parameters. The acceptance for this case is:</p>
<div class="math notranslate nohighlight">
\[\alpha  =   \min\left(1,\frac{\mathrm{p}\left(\mathbf d|\mathbf{x'},M'\right)\mathrm{p}\left(\mathbf{x'}|M'\right)\mathrm{p}\left(M'\right)}{\mathrm{p}\left(\mathbf d|\mathbf{x}_\mathrm{t},M_\mathrm{t}\right)\mathrm{p}\left(\mathbf{x}_\mathrm{t}|M_\mathrm{t}\right)\mathrm{p}\left(M_\mathrm{t}\right)}.\frac{\mathrm{j}\left(\mathbf{x'}\right)}{\mathrm{j}\left(\mathbf{x}_\mathrm{t}\right)\mathrm{q}\left(\mathbf u\right)}\left|\mathbf J\right|\right) .\]</div>
<p>If the jump is from higher dimensions to lower, the bijection describing the transformation has an inverse that describes the transformation <span class="math notranslate nohighlight">\(\mathbf{x'}\rightarrow\mathbf{x}_\mathrm{t},\mathbf u\)</span>, and the acceptance is given by:</p>
<div class="math notranslate nohighlight">
\[\alpha=\min\left( 1,\frac{\mathrm{p}\left(\mathbf d|\mathbf{x}_\mathrm{t},M_\mathrm{t}\right)\mathrm{p}\left(\mathbf{x}_\mathrm{t}|M_\mathrm{t}\right)\mathrm{p}\left(M_\mathrm{t}\right)}{\mathrm{p}\left(\mathbf d|\mathbf{x'},M'\right)\mathrm{p}\left(\mathbf{x'}|M'\right)\mathrm{p}\left(M'\right)}.\frac{\mathrm{j}\left(\mathbf{x}_\mathrm{t}\right)\mathrm{q}\left(\mathbf u\right)}{\mathrm{j}\left(\mathbf{x'}\right)}\left|\mathbf J^{-1}\right|\right) .\]</div>
<p>A simple example used in the literature (see Green, 1995; Brooks et al., 2003) is a mapping from a one dimensional model with parameter <span class="math notranslate nohighlight">\(\theta\)</span> to a two dimensional model with parameters <span class="math notranslate nohighlight">\(\theta_{1},\theta_{2}\)</span>. A possible bijection is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}h\left(\theta,u\right) =   \begin{cases}
\theta_{1} &amp; =\theta-u\\
\theta_{2} &amp; =\theta+u
\end{cases}\end{split}\]</div>
<p>with the reverse bijection given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}h\left(\theta_{1},\theta_{2}\right) =   \begin{cases}
\theta &amp; =\frac{1}{2}\left(\theta_{1}+\theta_{2}\right)\\
u &amp; =\frac{1}{2}\left(\theta_{1}-\theta_{2}\right)
\end{cases}\end{split}\]</div>
<div class="section" id="reversible-jump-mcmc-in-source-inversion">
<h4>8.4.1.1. Reversible Jump McMC in Source Inversion<a class="headerlink" href="#reversible-jump-mcmc-in-source-inversion" title="Permalink to this headline">¶</a></h4>
<p>The reversible jump McMC approach allows switching between different source models, such as the double-couple model and the higher dimensional model of the full moment tensor, and can be extended to other source models.</p>
<p>The full moment tensor model is nested around the double-couple point, leading to a simple description of the jumps by keeping the common parameters constant. The Tape parameterisation (<a class="reference internal" href="references.html#tape-2012"><span class="std std-ref">Tape and Tape, 2012</span></a>) allows for easy movement both in the source space and between models. The moment tensor model has five parameters:</p>
<blockquote>
<div><ul class="simple">
<li>strike <span class="math notranslate nohighlight">\(\left(\kappa\right)\)</span></li>
<li>dip cosine <span class="math notranslate nohighlight">\(\left(h\right)\)</span></li>
<li>slip <span class="math notranslate nohighlight">\(\left(\sigma\right)\)</span></li>
<li>eigenvalue co-latitude <span class="math notranslate nohighlight">\(\left(\delta\right)\)</span></li>
<li>eigenvalue longitude <span class="math notranslate nohighlight">\(\left(\gamma\right)\)</span></li>
</ul>
</div></blockquote>
<p>while the double-couple model has only the three orientation parameters: <span class="math notranslate nohighlight">\(\kappa\)</span>, <span class="math notranslate nohighlight">\(h\)</span>, and <span class="math notranslate nohighlight">\(\sigma\)</span>. Consequently, the orientation parameters are left unchanged between the two models, and the dimension balancing vector for the jump has two elements, which can be mapped to <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\delta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathrm{h}\left(\kappa_{\mathrm{DC}},h_{\mathrm{DC}},\sigma_{\mathrm{DC}},\mathbf u\right)\,\,\,\,\begin{cases}
\kappa_{\mathrm{MT}} &amp; =\kappa_{\mathrm{DC}}\\
h_{\mathrm{MT}} &amp; =h_{\mathrm{DC}}\\
\sigma_{\mathrm{MT}} &amp; =\sigma_{\mathrm{DC}}\\
\gamma_{\mathrm{MT}} &amp; =u_{1}\\
\delta_{\mathrm{MT}} &amp; =u_{2}
\end{cases}.\end{split}\]</div>
<p>The algorithm used is:</p>
<table border="1" class="docutils">
<colgroup>
<col width="3%" />
<col width="2%" />
<col width="94%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head" colspan="3"><strong>Reversible Jump Markov chain Monte Carlo sampling Algorithm</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>1</td>
<td colspan="2">Determine initial value for <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> with non-zero likelihood</td>
</tr>
<tr class="row-odd"><td rowspan="3">2</td>
<td colspan="2">Determine which move type to carry out:</td>
</tr>
<tr class="row-even"><td><ol class="first last loweralpha simple">
<li></li>
</ol>
</td>
<td>Carry out jump with probability <span class="math notranslate nohighlight">\(p_\mathrm{jump}\)</span>: Draw new sample
<span class="math notranslate nohighlight">\(\mathbf{x'}=\left(\mathbf{x},\mathbf{u}\right)\)</span> with dimension balancing vector <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> drawn from
<span class="math notranslate nohighlight">\(\mathrm{q}\left(u1, u2\right)\)</span>.</td>
</tr>
<tr class="row-odd"><td><ol class="first last loweralpha simple" start="2">
<li></li>
</ol>
</td>
<td>Carry out shift with probability <span class="math notranslate nohighlight">\(1-p_\mathrm{jump}\)</span>: Draw new sample <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span> from transition PDF
<span class="math notranslate nohighlight">\(\mathrm{q}\left(\mathbf{x'}|\mathbf{x}_\mathrm{t}\right)\)</span>.</td>
</tr>
<tr class="row-even"><td>3</td>
<td colspan="2">Evaluate likelihood for sample <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span></td>
</tr>
<tr class="row-odd"><td>4</td>
<td colspan="2">Calculate acceptance, <span class="math notranslate nohighlight">\(\alpha\)</span>, for <span class="math notranslate nohighlight">\(\mathbf{x'}\)</span>.</td>
</tr>
<tr class="row-even"><td>5</td>
<td colspan="2">Determine sample <span class="math notranslate nohighlight">\(\mathbf{x}_\mathrm{t+1}\)</span>.</td>
</tr>
<tr class="row-odd"><td rowspan="4">6</td>
<td colspan="2">If in learning period:</td>
</tr>
<tr class="row-even"><td><ol class="first last loweralpha simple">
<li></li>
</ol>
</td>
<td>If sufficient samples (&gt; 100) have been obtained, update transition PDF parameters to target ideal acceptance rate.</td>
</tr>
<tr class="row-odd"><td><ol class="first last loweralpha simple" start="2">
<li></li>
</ol>
</td>
<td>Return to 2 until end of learning period and discard learning samples.</td>
</tr>
<tr class="row-even"><td colspan="2">Otherwise return to 2 until sufficient samples are drawn.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="relative-amplitude">
<h2>8.5. Relative Amplitude<a class="headerlink" href="#relative-amplitude" title="Permalink to this headline">¶</a></h2>
<p>Inverting for multiple events increases the dimensions of the source space for each event. This leads to a much reduced probability of obtaining a non-zero likelihood sample, because sampling from the n-event distribution leads to multiplying the probabilities of drawing a non-zero samples, resulting in sparser sampling of the joint source PDF.</p>
<p>The elapsed time for the random sampling is longer per sample than the individual sampling, and longer than the combined sampling for both events due to
evaluating the relative amplitude PDF. Moreover, increasing the number of samples 10-fold raises the required time by a factor of 10, requiring some method of reducing the running time
for the inversion, since, given current processor speeds, <span class="math notranslate nohighlight">\(10^{15}\)</span> samples would take many years to calculate on a single core. As a result, more intelligent search algorithms are required for the full moment tensor case.</p>
<p>Markov chain approaches are less dependent on the model dimensionality. To account for the fact that the uncertainties in each parameter can differ between the events, the Markov
chain shape parameters can be scaled based on the relative non-zero percentages of the events when they are initialised. The initialisation approaches also need to be adjusted to
account for the reduced non-zero sample probability, such as by initialising the Markov chain independently for each event. The trans-dimensional McMC algorithm allows model
jumping independently for each event.</p>
<p>Tuning the Markov chain acceptance rate is difficult, as it is extremely sensitive to small changes in the proposal distribution widths, and with the higher dimensionality it may be
necessary to lower the targeted acceptance rate to improve sampling. Consequently, care needs to be taken when tuning the parameters to effectively implement the approaches for
relative amplitude data.</p>
</div>
<div class="section" id="running-time">
<h2>8.6. Running Time<a class="headerlink" href="#running-time" title="Permalink to this headline">¶</a></h2>
<p>Comparing different sample sizes shows that the McMC approaches require far fewer samples than random sampling. However, the random sampling algorithm is quick to calculate
the likelihood for a large number of samples, unlike the McMC approach, because of the extra computations in calculating the acceptance and obtaining new samples. Some optimisations
have been included in the McMC algorithms, including calculating the probability for multiple new samples at once, with sufficient samples that there is a high probability of containing
an accepted sample. This is more efficient than repeatedly updating the algorithm.</p>
<p>Despite these optimisations, the McMC approach is still much slower to reach comparable sample
sizes, and is slower than would be expected just given the targeted acceptance rate, because of the additional computational overheads:</p>
<div class="figure align-center" id="id3">
<span id="algorithm-run-time"></span><a class="reference internal image-reference" href="_images/algorithm_elapsed_time.png"><img alt="Scatter plot of the run times in hours for the different algorithms" src="_images/algorithm_elapsed_time.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-text"><em>Elapsed Time for different sample sizes of the random sampling algorithm and for McMC algorithms with different number of unique samples.</em></span></p>
</div>
<p>Including location uncertainty and model uncertainty in the forward model causes a rapid reduction of the available samples for a given amount of RAM and increases the number of
times the forward model must be evaluated, lengthening the time for sufficient sampling.</p>
<p>The location uncertainty has less of an effect on the McMC algorithms, since the number of samples being
tested at any given iteration are small. Consequently, as the random sampling approach becomes slower, the fewer
samples required to construct the Markov chain starts to produce good samples of the source PDF at comparable
times. However, there is an initial offset in the elapsed time for the Markov chain Monte Carlo approaches due to
the burn in and initialisation of the algorithm:</p>
<div class="figure align-center" id="id5">
<span id="algorithm-location-run-time"></span><a class="reference internal image-reference" href="_images/algorithm_elapsed_time_location.png"><img alt="Scatter plot of the run times in hours for the different algorithms" src="_images/algorithm_elapsed_time_location.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-text"><em>Elapsed Time for different sample sizes of the random sampling algorithm and for McMC algorithms with different number of unique samples. The velocity model and location uncertainty in the source was included with a one degree binning reducing the number of location samples from 50,000 to 5, 463.</em></span></p>
</div>
<p>The relative amplitude algorithms require an exponential increase in the number of samples as the number of events being sampled increases. However, the McMC approaches are more intelligent and do not require the same increase in sample size, but these algorithms can prove difficult to estimate the appropriate sampling parameters for the proposal distribution:</p>
<div class="figure align-center" id="id7">
<span id="relative-algorithm-run-time"></span><a class="reference internal image-reference" href="_images/relative_algorithm_elapsed_time.png"><img alt="Scatter plot of the run times in hours for the different algorithms" src="_images/relative_algorithm_elapsed_time.png" style="width: 50%;" /></a>
<p class="caption"><span class="caption-text"><em>Elapsed Time for different sample sizes of the random sampling algorithm and for McMC algorithms with different number of unique samples for a two event joint PDF with relative P-amplitudes.</em></span></p>
</div>
</div>
<div class="section" id="summary">
<h2>8.7. Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>There are several different approaches to sampling for the forward model. The Monte Carlo random sampling is quick, although it requires a large number of samples to produce good samplings of the PDF. The McMC approaches produce good sampling of the source for far fewer samples, but the evaluation time can be long due to the overhead of calculating the acceptance and generating new samples. Furthermore, these approaches rely on achieving the desired acceptance rate to sufficiently sample the PDF given the chain length.
Including location uncertainty increases the random sampling time drastically, and has less of an effect on the McMC approaches.</p>
<p>The solutions for the different algorithms and different source types are consistent, but the McMC approach can be poor at sampling multi-modal distributions if the acceptance rate is too high.</p>
<p>The two different approaches for estimating the probability of the double-couple source type model being correct are consistent, apart from cases where the PDF is dominated by a few very high probability samples. For these solutions, the number of random samples needs to be increased sufficiently to estimate the probability well. The McMC approach cannot easily be used to estimate the double-couple model probabilities, unlike both the random sampling and trans-dimensional approaches.</p>
<p>Extending these models to relative source inversion leads to a drastically increased number of samples required for the Monte Carlo random sampling approach that it becomes infeasible with current computation approaches. However, the McMC approaches can surmount this problem due to the improved sampling approach, but care must be taken when initialising the algorithm to make sure that sufficient samples are discarded that any initial bias has been removed, as well as targeting an acceptance rate that does not require extremely large chain lengths to successfully explore the full space.</p>
<p>The required number of samples depends on each event, and the constraint on the source that is given by the data. Usually including more data-types can sharpen the PDF, requiring more samples to get a satisfactory sampling of the event.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="mtconvert.html" class="btn btn-neutral float-right" title="9. convert" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="probability.html" class="btn btn-neutral" title="7. Probability" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, David Pugh.
      Last updated on Aug 30, 2018 (version 1.0.5).

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1.0.5',
            LANGUAGE:'English',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>